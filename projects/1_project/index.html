<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Visual odometry | Saharsh Barve</title> <meta name="author" content="Saharsh Barve"> <meta name="description" content="Evaluated classical stereo vision and deep learning-based methods for visual odometry on KITTI dataset, analyzing their efficacy in calculating depth maps and tracking motion."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/thumbnail.png?1817f4525315c84c8838a565914d9e75"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://saharsh1005.github.io/projects/1_project/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Saharsh Barve</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Visual odometry</h1> <p class="post-description">Evaluated classical stereo vision and deep learning-based methods for visual odometry on KITTI dataset, analyzing their efficacy in calculating depth maps and tracking motion.</p> </header> <article> <h1 id="stereo-vision-and-visual-odometry-project-with-kitti-dataset">Stereo Vision and Visual Odometry Project with KITTI Dataset</h1> <p><a href="https://github.com/your_username/your_project/releases/tag/v1.0" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/badge/Version-1.0-brightgreen" alt="Version 1.0"></a> <a href="https://github.com/your_username/your_project/commit/your_commit_id" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/badge/Created%20on-December%202024-blue" alt="Created on December 2024"></a></p> <h2 id="group-members">Group Members</h2> <ul> <li> <strong>Harris Nisar</strong>: GitHub - <a href="https://github.com/nisar2" rel="external nofollow noopener" target="_blank">nisar2</a> </li> <li> <strong>Saharsh Sandeep Barve</strong>: GitHub - <a href="https://github.com/Saharsh1005" rel="external nofollow noopener" target="_blank">ssbarve2</a> </li> </ul> <h2 id="table-of-contents">Table of Contents</h2> <ul> <li><a href="#project-description--goals">Project Description &amp; Goals</a></li> <li><a href="#member-roles">Member Roles</a></li> <li><a href="#resources">Resources</a></li> <li><a href="#results">Results</a></li> <li><a href="#background">Background</a></li> <li><a href="#acknowledgement">Acknowledgement</a></li> <li><a href="#references">References</a></li> </ul> <h2 id="project-description--goals">Project Description &amp; Goals</h2> <h3 id="description">Description</h3> <p>Reconstructing 3D information from images has many applications in robotics and autonomous vehicles. For example, it is crucial for autonomous vehicles to know their position in 3D space. Uncovering such localization information can be achieved through visual odometry, in which the pose of the camera is tracked as it moves through an environment. Visual odometry is often accomplished through stereo vision for depth estimation. Stereo vision can be used to determine the 3D coordinates of features in stereo pairs taken over time. Then, for each subsequent pair, the transformation that best describes the change in transformations is computed, thereby allowing to track the camera through the environment. In this project, we aim to understand stereo vision, and subsequently understand visual odometry, following two approaches classical stereo vision and deep learning based stereo vision.</p> <p><img src="/assets/img/project_preview/flowchart_diagram.png" alt="Project Framework"></p> <h3 id="goals">Goals</h3> <h4 id="preliminaries">Preliminaries:</h4> <ol> <li> <strong>Understanding the KITTI datasets:</strong> Both team members will collaborate to familiarize themselves with the KITTI stereo and visual odometry datasets.</li> <li> <strong>Understanding basic mathematics and algorithms for stereo and visual odometry:</strong> Both team members will acquire a strong theoretical foundation in 3D vision techniques.</li> </ol> <h4 id="implement-traditional-stereo-vision">Implement traditional stereo vision:</h4> <ol> <li> <strong>Implement traditional stereo vision:</strong> Both team members will work together to implement traditional stereo vision techniques using tutorials and resources.</li> <li> <strong>Test on KITTI stereo and visual odometry datasets:</strong> The implemented traditional stereo vision will be tested on the KITTI datasets.</li> <li> <strong>Implement visual odometry:</strong> Both team members will work on implementing visual odometry using tutorials.</li> </ol> <h4 id="implement-deep-learning-for-stereo-vision">Implement deep learning for stereo vision:</h4> <ol> <li> <strong>Explore and run the deep learning model on the stereo dataset:</strong> Harris will lead this effort, exploring and running deep learning models on the stereo dataset.</li> <li> <strong>Explore and run the deep learning model on the odometry dataset:</strong> Saharsh will lead this part of the project, exploring and running deep learning models on the odometry dataset.</li> <li> <strong>Replace traditional stereo with deep learning in visual odometry and compare performance:</strong> Both team members will collaborate to replace traditional stereo with deep learning in the visual odometry pipeline and evaluate their respective performances using labeled sequences from the KITTI dataset.</li> </ol> <h2 id="member-roles">Member Roles</h2> <ul> <li> <strong>Harris Nisar:</strong> <ul> <li>Explore and run the deep learning model on the stereo dataset.</li> </ul> </li> <li> <strong>Saharsh Sandeep Barve:</strong> <ul> <li>Explore and run the deep learning model on the odometry dataset.</li> </ul> </li> </ul> <p>Both team members will collaborate on other aspects of the project, including understanding the datasets, basic mathematics, implementing traditional stereo vision, implementing visual odometry, and comparing the impact of the two stereo vision approaches for visual odometry.</p> <h2 id="resources">Resources</h2> <h3 id="data">Data:</h3> <ul> <li> <strong>KITTI Stereo Dataset:</strong> Contains 200 training scenes and 200 test scenes with four color images per scene, saved in lossless PNG format. Ground truth data is available for evaluation.</li> <li> <strong>KITTI Visual Odometry Dataset:</strong> Consists of 22 stereo sequences in lossless PNG format, with ground truth trajectories available for 11 training sequences (00-10).</li> </ul> <h3 id="implementation-platform">Implementation Platform:</h3> <ul> <li> <strong>Language:</strong> Python</li> <li> <strong>Libraries:</strong> OpenCV, PIL, Numpy, PyTorch</li> </ul> <h3 id="resources-and-tutorials">Resources and Tutorials:</h3> <ul> <li><a href="https://github.com/savnani5/Depth-Estimation-using-Stereovision" rel="external nofollow noopener" target="_blank">Stereo Vision 1</a></li> <li><a href="https://github.com/FoamoftheSea/KITTI_visual_odometry/tree/main" rel="external nofollow noopener" target="_blank">Visual Odometry 1</a></li> <li><a href="https://jasleon.github.io/Visual-Odometry/" rel="external nofollow noopener" target="_blank">Visual Odometry 2</a></li> <li><a href="https://www.cs.toronto.edu/~urtasun/publications/luo_etal_cvpr16.pdf" rel="external nofollow noopener" target="_blank">Efficient Deep Learning for Stereo Matching</a></li> <li>Deep Learning for Depth Estimation: <ul> <li><a href="https://github.com/isl-cv/chitransformer" rel="external nofollow noopener" target="_blank">Repo 1</a></li> <li><a href="https://github.com/datvuthanh/Stereo-Matching" rel="external nofollow noopener" target="_blank">Repo 2</a></li> </ul> </li> </ul> <h3 id="computational-resources">Computational Resources:</h3> <p>The project requires substantial computational resources, and the team has access to suitable hardware, including an RTX-3070 GPU, for both model training and inference.</p> <h2 id="project-results">Project Results</h2> <h3 id="project-result-video">Project result video:</h3> <p><a href="https://youtu.be/cJ4hgQHH6Ac" rel="external nofollow noopener" target="_blank">Link to Project Results Video</a></p> <h3 id="raw-images">Raw Images</h3> <p><strong>Left Camera Image</strong> <img src="/assets/img/project_preview/raw_image_left.png" alt="Raw Image - Left Camera"></p> <p><strong>Right Camera Image</strong> <img src="/assets/img/project_preview/raw_image_right.png" alt="Raw Image - Right Camera"></p> <h3 id="disparity-map-sgbm-from-classical-stereo-vision">Disparity Map SGBM from Classical Stereo Vision</h3> <p><img src="/assets/img/project_preview/classic_sgbm_disparity_map.png" alt="Disparity Map - Classical Stereo"></p> <h3 id="depth-from-deep-learning-stereo-vision">Depth from Deep Learning Stereo Vision</h3> <p><img src="/assets/img/project_preview/dl_stereo_depthmap.png" alt="Deep Learning Stereo Depth Map"></p> <h3 id="odometry-results">Odometry Results</h3> <h4 id="classical-stereo-vision">Classical Stereo Vision</h4> <ol> <li><img src="/assets/img/project_preview/classic_00_odometry.png" alt="Odometry 00"></li> <li><img src="/assets/img/project_preview/classic_02_odometry.png" alt="Odometry 02"></li> <li><img src="/assets/img/project_preview/classic_05_odometry.png" alt="Odometry 05"></li> </ol> <h4 id="deep-learning-stereo-vision">Deep Learning Stereo Vision</h4> <ol> <li><img src="/assets/img/project_preview/dl_00_odometry.png" alt="Odometry 00"></li> <li><img src="/assets/img/project_preview/dl_02_odometry.png" alt="Odometry 02"></li> <li><img src="/assets/img/project_preview/dl_05_odometry.png" alt="Odometry 05"></li> </ol> <h3 id="mean-squared-error-mse-error">Mean Squared Error (MSE) Error</h3> <p><img src="/assets/img/project_preview/mse_error.png" alt="MSE Error"></p> <p>The above images showcase the results obtained from classical stereo vision, deep learning stereo vision, and the raw images used in the visual odometry pipeline. The odometry results for different sequences are presented for both classical and deep learning stereo vision. Additionally, the Mean Squared Error (MSE) error in depth estimation is visualized to evaluate the performance of the stereo vision methods.</p> <p>For a more detailed analysis and discussion of the results, please refer to the corresponding sections in the report.</p> <h2 id="background">Background</h2> <p>This project is significant for the team as it aligns with their work on a real-time medical instrument tracking system, which involves intelligent simulations and utilizes stereo vision techniques. Although the dataset and scale differ (cars vs. medical tools), the primary objective is to gain theoretical and practical expertise in 3D vision techniques, demystifying these complex algorithms. Additionally, implementing these algorithms in Python using familiar libraries like OpenCV and PyTorch will establish a solid foundation for their research work.</p> <h2 id="acknowledgement">Acknowledgement</h2> <p>We’d also like to acknowledge the work at <a href="https://github.com/FoamoftheSea/KITTI_visual_odometry" rel="external nofollow noopener" target="_blank">this</a> repository for making the concepts of visual odometry easy to understand and providing a lot of code that we reused for the pipeline.</p> <h2 id="references">References</h2> <p>[1] D. Scaramuzza and F. Fraundorfer, “Visual Odometry [Tutorial],” IEEE Robot. Autom. Mag., vol. 18, no. 4, pp. 80–92, Dec. 2011, doi: 10.1109/MRA.2011.943233.</p> <p>[2] H. Moravec, “Obstacle Avoidance and Navigation in the Real World by a Seeing Robot Rover.” Sep. 1980. [Online]. Available: <a href="https://www.ri.cmu.edu/pub_files/pub4/moravec_hans_1980_1/moravec_hans_1980_1.pdf" rel="external nofollow noopener" target="_blank">Link to PDF</a></p> <p>[3] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? The KITTI vision benchmark suite,” in 2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI: IEEE, Jun. 2012, pp. 3354–3361. doi: 10.1109/CVPR.2012.6248074.</p> <p>[4] Q. Su and S. Ji, “ChiTransformer:Towards Reliable Stereo from Cues,” 2022, doi: 10.48550/ARXIV.2203.04554.</p> <p>[5] G. Bradski, “The OpenCV library,” Dr Dobbs J. Softw. Tools, 2000.</p> <p>[6] D. G. Lowe, “Distinctive Image Features from Scale-Invariant Keypoints,” Int. J. Comput. Vis., vol. 60, no. 2, pp. 91–110, Nov. 2004, doi: 10.1023/B:VISI.0000029664.99615.94.</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Saharsh Barve. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: October 22, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>